{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b8b3bf",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "\n",
    "1. Ensemble learning\n",
    "1. bootstrap aggregation\n",
    "1. gradient descent\n",
    "1. Boosting, Boosting Trees\n",
    "1. Gradient Boosting, gradient boosted decision trees\n",
    "1. XGBoost\n",
    "\n",
    "\n",
    "# Resources\n",
    "\n",
    "\n",
    "1. [A Beginner’s guide to XGBoost\n",
    "](https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7)\n",
    "1. [XGBoost: Everything You Need to Know\n",
    "](https://neptune.ai/blog/xgboost-everything-you-need-to-know#:~:text=XGBoost%20is%20a%20popular%20gradient,it's%20very%20easy%20to%20use.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0ed18",
   "metadata": {},
   "source": [
    "# Ensemble algorithms\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Ensemble learning** combines several learners (models) to improve overall performance, increasing predictiveness and accuracy in machine learning and predictive modeling.\n",
    "\n",
    "Technically speaking, the power of ensemble models is simple: it can combine thousands of smaller learners trained on subsets of the original data. This can lead to interesting observations that, like:\n",
    "\n",
    "- The variance of the general model decreases significantly thanks to **bagging**\n",
    "- The bias also decreases due to **boosting** \n",
    "- And overall predictive power improves because of **stacking**  \n",
    "\n",
    "## Types of ensemble methods\n",
    "\n",
    "Ensemble methods can be classified into two groups based on how the sub-learners are generated:\n",
    "\n",
    "1. **Sequential ensemble methods** – learners are generated sequentially. These methods use the dependency between base learners. Each learner influences the next one, likewise, a general paternal behavior can be deduced. A popular example of sequential ensemble algorithms is **AdaBoost**. \n",
    "\n",
    "1. **Parallel ensemble methods** – learners are generated in parallel. The base learners are created independently to study and exploit the effects related to their independence and reduce error by averaging the results. An example implementing this approach is **Random Forest.**\n",
    "\n",
    "\n",
    "## Homogenous and heterogenous ML algorithms  \n",
    "\n",
    "- Ensemble methods can use **homogeneous learners** (learners from the same family) or **heterogeneous learners** (learners from multiple sorts, as accurate and diverse as possible).\n",
    "\n",
    "Generally speaking, homogeneous ensemble methods have a single-type base learning algorithm. The training data is diversified by assigning weights to training samples, but they usually leverage a single type base learner. \n",
    "\n",
    "Heterogeneous ensembles on the other hand consist of members having different base learning algorithms which can be combined and used simultaneously to form the predictive model. \n",
    "\n",
    "A general rule of thumb: \n",
    "\n",
    "- **Homogeneous ensembles** use the same feature selection with a variety of data and distribute the dataset over several nodes. Homogeneous Ensembles:\n",
    "    - Ensemble algorithms that use bagging like Decision Trees Classifiers\n",
    "    - Random Forests, Randomized Decision Trees\n",
    "\n",
    "- **Heterogeneous ensembles** use different feature selection methods with the same data. Heterogeneous Ensembles:\n",
    "\n",
    "    - Support Vector Machines, SVM\n",
    "    - Artificial Neural Networks, ANN\n",
    "    - Memory-Based Learning methods\n",
    "    - Bagged and Boosted decision Trees like XGBoost\n",
    "\n",
    "\n",
    "## Bagging\n",
    "\n",
    "**Decrease overall variance** by averaging the performance of multiple estimates. Aggregate several sampling subsets of the original dataset to train different learners chosen randomly with replacement, which conforms to the core idea of bootstrap aggregation. Bagging normally uses a voting mechanism for **classification** (Random Forest) and averaging for **regression**.\n",
    "\n",
    "<img width=\"977\" alt=\"image\" src=\"https://github.com/eraikakou/ml-theory/assets/28102493/99049aed-daef-463c-898a-e872c372b296\">\n",
    "\n",
    "\n",
    "- **Note:** Remember that some learners are stable and less sensitive to training perturbations. Such learners, when combined, don’t help the general model to improve generalization performance.\n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "This technique matches weak learners — learners that have poor predictive power and do slightly better than random guessing — to a specific weighted subset of the original dataset. Higher weights are given to subsets that were misclassified earlier.\n",
    "\n",
    "Learner predictions are then combined with voting mechanisms in case of classification or weighted sum for regression.\n",
    "\n",
    "\n",
    "<img width=\"932\" alt=\"image\" src=\"https://github.com/eraikakou/ml-theory/assets/28102493/72e432c5-90a3-4b44-bcb0-9e8212cd2717\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922608c9",
   "metadata": {},
   "source": [
    "# Boosting, Boosting Trees\n",
    "\n",
    "## Introduction\n",
    "\n",
    "With a regular machine learning model, like a decision tree, we’d simply train a single model on our dataset and use that for prediction. We might play around with the parameters for a bit or augment the data, but in the end we are still using a single model. Even if we build an ensemble, all of the models are trained and applied to our data separately.\n",
    "\n",
    "Boosting, on the other hand, takes a more iterative approach. It’s still technically an ensemble technique in that many models are combined together to perform the final one, but takes a more clever approach.\n",
    "\n",
    "***Rather than training all of the models in isolation of one another, boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones.*** Models are added sequentially until no further improvements can be made.\n",
    "\n",
    "The advantage of this iterative approach is that the new models being added are focused on correcting the mistakes which were caused by other models. **In a standard ensemble method where models are trained in isolation, all of the models might simply end up making the same mistakes!**\n",
    "\n",
    "\n",
    "## Well-known boosting algorithms \n",
    "\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost stands for **Adaptive Boosting**. The logic implemented in the algorithm is: \n",
    "\n",
    "1. First-round classifiers (learners) are all trained using weighted coefficients that are equal,\n",
    "\n",
    "1. In subsequent boosting rounds the adaptive process increasingly weighs data points that were misclassified by the learners in previous rounds and decrease the weights for correctly classified ones. \n",
    "\n",
    "If you’re curious about the algorithm’s description, take a look at this:\n",
    "\n",
    "<img width=\"723\" alt=\"image\" src=\"https://github.com/eraikakou/ml-theory/assets/28102493/525b6992-4b71-4de7-9ff2-86ec96077054\">\n",
    "\n",
    "### Gradient Boosting methods\n",
    "\n",
    "Gradient Boosting specifically is an approach where new models are trained to predict the residuals (i.e errors) of prior models. I’ve outlined the approach in the diagram below.\n",
    "\n",
    "\n",
    "<img width=\"787\" alt=\"image\" src=\"https://github.com/eraikakou/ml-theory/assets/28102493/20f21c25-597e-4170-9fcd-0d7a0817696e\">\n",
    "\n",
    "**Gradient Boosting uses differentiable function losses from the weak learners to generalize. At each boosting stage, the learners are used to minimize the loss function given the current model.** Boosting algorithms can be used either for **classification or regression**. \n",
    "\n",
    "\n",
    "<img width=\"550\" alt=\"image\" src=\"https://github.com/eraikakou/ml-theory/assets/28102493/86fd9b7a-5594-4c02-b32b-88c878ba407d\">\n",
    "\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "XGBoost stands for Extreme Gradient Boosting. It’s a parallelized and carefully optimized version of the gradient boosting algorithm. Parallelizing the whole boosting process hugely improves the training time. \n",
    "\n",
    "Instead of training the best possible model on the data (like in traditional methods), we train thousands of models on various subsets of the training dataset and then vote for the best-performing model.\n",
    "\n",
    "For many cases, XGBoost is better than usual gradient boosting algorithms. The Python implementation gives access to a vast number of inner parameters to tweak for better precision and accuracy.\n",
    "\n",
    "Some important features of XGBoost are:\n",
    "\n",
    "- **Parallelization:** The model is implemented to train with multiple CPU cores.\n",
    "\n",
    "- **Regularization:** XGBoost includes different regularization penalties to avoid overfitting. Penalty regularizations produce successful training so the model can generalize adequately.\n",
    "\n",
    "- **Non-linearity:** XGBoost can detect and learn from non-linear data patterns.\n",
    "\n",
    "- **Cross-validation:** Built-in and comes out-of-the-box.\n",
    "\n",
    "- **Scalability:** XGBoost can run distributed thanks to distributed servers and clusters like Hadoop and Spark, so you can process enormous amounts of data. It’s also available for many programming languages like C++, JAVA, Python, and Julia. \n",
    "\n",
    "\n",
    "#### Gradient Boosting Machine (GBM)\n",
    "\n",
    "**GBM** combines predictions from multiple decision trees, and all the weak learners are decision trees. The key idea with this algorithm is that every node of those trees takes a different subset of features to select the best split. As it’s a Boosting algorithm, each new tree learns from the errors made in the previous ones.\n",
    "\n",
    "\n",
    "#### Light Gradient Boosting Machine (LightGBM)\n",
    "\n",
    "**LightGBM** can handle huge amounts of data. It’s one of the fastest algorithms for both training and prediction. It generalizes well, meaning that it can be used to solve similar problems. It scales well to large numbers of cores and has an open-source code so you can use it in your projects for free.\n",
    "\n",
    "#### Categorical Boosting (CatBoost)\n",
    "\n",
    "This particular set of Gradient Boosting variants has specific abilities to handle categorical variables and data in general. The **CatBoost** object can handle categorical variables or numeric variables, as well as datasets with mixed types. That’s not all. It can also use unlabelled examples and explore the effect of kernel size on speed during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee1f0e",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "## Introduction\n",
    "\n",
    "XGBoost is an open source library providing a high-performance implementation of **gradient boosted decision trees**. An underlying **C++ codebase combined with a Python interface** sitting on top makes for an extremely powerful yet easy to implement package. XGBoost is a popular gradient-boosting library for GPU training, distributed computing, and parallelization. It’s precise, it adapts well to all types of data and problems, it has excellent documentation, and overall it’s very easy to use. \n",
    "\n",
    "The performance of XGBoost is no joke — it’s become the go-to library for winning many Kaggle competitions. Its gradient boosting implementation is second to none and there’s only more to come as the library continues to garner praise.\n",
    "\n",
    "At the moment it’s the de facto standard algorithm for getting accurate results from predictive modeling with machine learning. It’s the fastest gradient-boosting library for R, Python, and C++ with very high accuracy.\n",
    "\n",
    "\n",
    "## Getting started with XGBoost\n",
    "\n",
    "\n",
    "1. **Step 1:** In order for XGBoost to be able to use our data, we’ll need to transform it into a specific format that XGBoost can handle. That format is called `DMatrix`. It’s a very simple one-linear to transform a numpy array of data to DMatrix format:\n",
    "    - `D_train = xgb.DMatrix(X_train, label=Y_train)`\n",
    "    - `D_test = xgb.DMatrix(X_test, label=Y_test)`\n",
    "\n",
    "1. **Step 2 - Defining an XGBoost model:** Now that our data is all loaded up, we can define the parameters of our gradient boosting ensemble. We’ve set up some of the most important ones below to get us started. For more complicated tasks and models, the full list of possible parameters is available on the official XGBoost website. The simplest parameters are the:\n",
    "\n",
    "    - `max_depth` (maximum depth of the decision trees being trained), \n",
    "    - `objective` (the loss function being used), and \n",
    "    - `num_class` (the number of classes in the dataset). \n",
    "    - The `eta` algorithm requires special attention. From our theory, Gradient Boosting involves creating and adding decision trees to an ensemble model sequentially. New trees are created to correct the residual errors in the predictions from the existing ensemble. **Due to the nature of an ensemble, i.e having several models put together to form what is essentially a very large complicated one, makes this technique prone to overfitting. The eta parameter gives us a chance to prevent this overfitting.** The eta can be thought of more intuitively as a learning rate. Rather than simply adding the predictions of new trees to the ensemble with full weight, the eta will be multiplied by the residuals being adding to reduce their weight. This effectively reduces the complexity of the overall model. It is common to have small values in the range of 0.1 to 0.3. The smaller weighting of these residuals will still help us train a powerful model, but won’t let that model run away into deep complexity where overfitting is more likely to happen.\n",
    "    - `steps` The number of training iterations\n",
    "    - But there are some more cool features that’ll help you get the most out of your models. The `gamma` parameter can also help with controlling overfitting. It specifies the minimum reduction in the loss required to make a further partition on a leaf node of the tree. I.e if creating a new node doesn’t reduce the loss by a certain amount, then we won’t create it at all.\n",
    "    - The `booster` parameter allows you to set the type of model you will use when building the ensemble. The default is gbtree which builds an ensemble of decision trees. If your data isn’t too complicated, you can go with the faster and simpler gblinear option which builds an ensemble of linear models.\n",
    "\n",
    "1. **Step 3 - Grid Search:** Setting the optimal hyperparameters of any ML model can be a challenge. So why not let Scikit Learn do it for you? We can combine Scikit Learn’s grid search with an XGBoost classifier quite easily. **Only do that on a big dataset if you have time to kill — doing a grid search is essentially training an ensemble of decision trees many times over!**\n",
    "\n",
    "1. **Step 4 - Training and Testing:**  We can finally train our model similar to how we do so with Scikit Learn: `model = xgb.train(param, D_train, steps)`\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "parameters = {\n",
    "     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "     }\n",
    "\n",
    "grid = GridSearchCV(clf,\n",
    "                    parameters, n_jobs=4,\n",
    "                    scoring=\"neg_log_loss\",\n",
    "                    cv=3)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "```\n",
    "\n",
    "## How does the XGBoost algorithm work?\n",
    "\n",
    "- Consider a function or estimate . To start, we build a sequence derived from the function gradients. The equation below models a particular form of gradient descent. The represents the Loss function to minimize hence it gives the direction in which the function decreases. is the rate of change fitted to the loss function, it’s equivalent to the learning rate in gradient descent. is expected to approximate the behaviour of the loss suitably.\n",
    "    \n",
    "    <img width=\"424\" alt=\"image\" src=\"https://github.com/eraikakou/ml-theory/assets/28102493/8d409265-2140-4ccb-8152-198ba869da7f\">\n",
    "    \n",
    "- To iterate over the model and find the optimal definition we need to express the whole formula as a sequence and find an effective function that will converge to the minimum of the function. This function will serve as an error measure to help us decrease the loss and keep the performance over time. The sequence converges to the minimum of the function . This particular notation defines the error function that applies when evaluating a gradient boosting regressor. \n",
    "    \n",
    "    <img width=\"530\" alt=\"image\" src=\"https://github.com/eraikakou/ml-theory/assets/28102493/5a6a2353-333c-42ba-9d99-93c71873d3c4\">\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
